{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic \n",
    "\n",
    "make all the comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: certifi in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: requests in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (2.25.0)\n",
      "Requirement already satisfied: tqdm in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (4.54.1)\n",
      "Requirement already satisfied: urllib3 in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: six>=1.10 in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\n",
      "Collecting python-slugify\n",
      "  Downloading python-slugify-4.0.1.tar.gz (11 kB)\n",
      "Requirement already satisfied: certifi in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/denia/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: urllib3 in /home/denia/anaconda3/lib/python3.8/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/denia/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.2 MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=c062187a59bb240cb22b2dfed5f96b0a9b611ce6420ab38d15c023f2eb8211e3\n",
      "  Stored in directory: /home/denia/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "  Building wheel for python-slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-slugify: filename=python_slugify-4.0.1-py2.py3-none-any.whl size=6768 sha256=7213223d83db631b48ebfad83875d8b5c8e33b4067b6f9eae92dfa3531c5deb9\n",
      "  Stored in directory: /home/denia/.cache/pip/wheels/91/4d/4f/e740a68c215791688c46c4d6251770a570e8dfea91af1acb5c\n",
      "Successfully built kaggle python-slugify\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.5.12 python-slugify-4.0.1 text-unidecode-1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!cd ~/.kaggle/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/denia/.kaggle/kaggle.json'\n",
      "ref                                                               title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
      "----------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
      "shuofxz/titanic-machine-learning-from-disaster                    Titanic: Machine Learning from Disaster           33KB  2017-10-15 10:05:34           2156         38  0.29411766       \n",
      "rashigoel/titanic-machine-learning-from-disaster                  Titanic: Machine Learning from Disaster           34KB  2018-02-10 17:53:45            753         10  0.29411766       \n",
      "vstepanenko/disaster-tweets                                       Disaster Tweets                                  656KB  2020-11-12 14:56:58           1062         79  1.0              \n",
      "harunshimanto/titanic-solution-a-beginners-guide                  Titanic Solution: A Beginner's Guide              35KB  2018-03-11 14:23:57            813         10  0.5294118        \n",
      "christianlillelund/passenger-list-for-the-estonia-ferry-disaster  The Estonia Disaster Passenger List               14KB  2020-07-26 15:40:17           1434        113  1.0              \n",
      "azeembootwala/titanic                                             Titanic                                           12KB  2017-06-05 12:14:37           8921        104  0.8235294        \n",
      "sweetyparmar1/titanic                                             Titanic:Machine Learning From Disaster            34KB  2017-12-25 19:53:34            368          7  0.29411766       \n",
      "dryad/human-mobility-during-natural-disasters                     Human Mobility During Natural Disasters           49MB  2017-08-30 21:39:34           1323         44  0.8235294        \n",
      "pavlofesenko/titanic-extended                                     Titanic extended dataset (Kaggle + Wikipedia)    134KB  2019-03-06 09:53:24           5882        104  0.9411765        \n",
      "rahulsah06/titanic                                                Titanic                                           34KB  2019-09-16 14:43:23           1095         18  0.6764706        \n",
      "headsortails/us-natural-disaster-declarations                     US Natural Disaster Declarations                   2MB  2021-04-08 15:38:21            858         58  1.0              \n",
      "jannesklaas/disasters-on-social-media                             Disasters on social media                        755KB  2018-04-15 07:04:59           1554         23  0.64705884       \n",
      "vbmokin/nlp-with-disaster-tweets-cleaning-data                    NLP with Disaster Tweets - cleaning data         515KB  2020-07-12 18:07:38            282         19  1.0              \n",
      "fema/federal-disasters                                            Federal Emergencies and Disasters, 1953-Present  374KB  2017-02-19 02:30:20           1351         35  0.8235294        \n",
      "arminehn/disasteraccident-sources                                 Disaster/Accident Sources                          1MB  2017-05-27 03:02:48            841         21  0.8235294        \n",
      "criticalhits/canadian-disaster-database                           Canadian Disaster Database                       625KB  2017-09-29 21:16:38           1403         45  0.85294116       \n",
      "prkukunoor/TitanicDataset                                         Titanic                                          135KB  2017-01-03 22:01:13           3953         22  0.5882353        \n",
      "ashishpatel26/titanicdisaster                                     Titanic: Machine Learning from Disaster           33KB  2018-03-21 10:44:07            142          0  0.29411766       \n",
      "dataenergy/natural-disaster-data                                  Natural disaster data                              7KB  2019-04-16 11:04:32           1179         12  0.5294118        \n",
      "landlord/multilingual-disaster-response-messages                  Multilingual Disaster Response Messages            2MB  2020-07-25 18:18:31             99         11  1.0              \n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list -s 'Titanic - Machine Learning from Disaster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/denia/.kaggle/kaggle.json'\n",
      "Downloading titanic.zip to /home/denia/Documents/Private_Projects/Kaggle/Titanic\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 2.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  titanic.zip\n",
      "  inflating: gender_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!unzip titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=pd.read_csv('test.csv')\n",
    "traindata=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "traindata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "testdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=pd.concat([traindata,testdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1309 non-null   int64  \n",
      " 1   Survived     891 non-null    float64\n",
      " 2   Pclass       1309 non-null   int64  \n",
      " 3   Name         1309 non-null   object \n",
      " 4   Sex          1309 non-null   object \n",
      " 5   Age          1046 non-null   float64\n",
      " 6   SibSp        1309 non-null   int64  \n",
      " 7   Parch        1309 non-null   int64  \n",
      " 8   Ticket       1309 non-null   object \n",
      " 9   Fare         1308 non-null   float64\n",
      " 10  Cabin        295 non-null    object \n",
      " 11  Embarked     1307 non-null   object \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 132.9+ KB\n"
     ]
    }
   ],
   "source": [
    "alldata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
